{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans Clustering\n",
    "\n",
    "Use KMeans clustering on our arrays of poses to get a limited vocabulary to feed into the language model. Ultimately, we will want about 50k poses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time, cv2, math, json, glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from D2M takes a numpy array, \"pose\" of the 14 D2M keypoints,\n",
    "# saves a rendered figure as \"outfile\".\n",
    "def vis_single(pose, outfile):\n",
    "  colors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0], \\\n",
    "          [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255], \\\n",
    "          [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]\n",
    "\n",
    "  # find connection in the specified sequence, center 29 is in the position 15\n",
    "  limbSeq = [[2,3], [2,6], [3,4], [4,5], [6,7], [7,8], [2,9], [9,10], \\\n",
    "           [10,11], [2,12], [12,13], [13,14], [2,1], [1,15], [15,17], \\\n",
    "           [1,16], [16,18], [3,17], [6,18]]\n",
    "\n",
    "  neglect = [14,15,16,17]\n",
    "\n",
    "  for t in range(1):\n",
    "    #break\n",
    "    canvas = np.ones((256,500,3), np.uint8)*255\n",
    "\n",
    "    thisPeak = pose\n",
    "    for i in range(18):\n",
    "      if i in neglect:\n",
    "        continue\n",
    "      if thisPeak[i,0] == -1:\n",
    "        continue\n",
    "      cv2.circle(canvas, tuple(thisPeak[i,0:2].astype(int)), 4, colors[i], thickness=-1)\n",
    "\n",
    "    for i in range(17):\n",
    "      limbid = np.array(limbSeq[i])-1\n",
    "      if limbid[0] in neglect or limbid[1] in neglect:\n",
    "        continue\n",
    "      X = thisPeak[[limbid[0],limbid[1]], 1]\n",
    "      Y = thisPeak[[limbid[0],limbid[1]], 0]\n",
    "      if X[0] == -1 or Y[0]==-1 or X[1]==-1 or Y[1]==-1:\n",
    "        continue\n",
    "      stickwidth = 4\n",
    "      cur_canvas = canvas.copy()\n",
    "      mX = np.mean(X)\n",
    "      mY = np.mean(Y)\n",
    "      length = ((X[0] - X[1]) ** 2 + (Y[0] - Y[1]) ** 2) ** 0.5\n",
    "      angle = math.degrees(math.atan2(X[0] - X[1], Y[0] - Y[1]))\n",
    "      polygon = cv2.ellipse2Poly((int(mY),int(mX)), (int(length/2), stickwidth), int(angle), 0, 360, 1)\n",
    "      cv2.fillConvexPoly(cur_canvas, polygon, colors[i])\n",
    "      canvas = cv2.addWeighted(canvas, 0.4, cur_canvas, 0.6, 0)\n",
    "    cv2.imwrite(outfile,canvas)\n",
    "    return canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from Stephanie's notebook.\n",
    "def make_video(images, outvid=None, fps=5, size=None,\n",
    "               is_color=True, format='MP42'):\n",
    "    \"\"\"\n",
    "    Create a video from a list of images.\n",
    " \n",
    "    @param      outvid      output video\n",
    "    @param      images      list of images to use in the video\n",
    "    @param      fps         frame per second\n",
    "    @param      size        size of each frame\n",
    "    @param      is_color    color\n",
    "    @param      format      see http://www.fourcc.org/codecs.php\n",
    "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
    " \n",
    "    The function relies on http://opencv-python-tutroals.readthedocs.org/en/latest/.\n",
    "    By default, the video will have the size of the first image.\n",
    "    It will resize every image to this size before adding them to the video.\n",
    "    MODIFIED FROM: http://www.xavierdupre.fr/blog/2016-03-30_nojs.html\n",
    "    \"\"\"\n",
    "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
    "    fourcc = VideoWriter_fourcc(*format)\n",
    "    vid = None\n",
    "    for image in images:\n",
    "        #print(image)\n",
    "        if not os.path.exists(image):\n",
    "            raise FileNotFoundError(image)\n",
    "        img = imread(image)\n",
    "        if vid is None:\n",
    "            if size is None:\n",
    "                size = img.shape[1], img.shape[0]\n",
    "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
    "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
    "            img = resize(img, size)\n",
    "        vid.write(img)\n",
    "    vid.release()\n",
    "    return vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_poses(dataset, dataset_name, k):\n",
    "    #Normalize arrays to 0 to 1 range.\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = scaler.fit(dataset)\n",
    "    data =  scaler.transform(dataset)\n",
    "\n",
    "    # Run KMeans with k clusters.\n",
    "    start_time = time.time()\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    clusters = kmeans.fit(data)\n",
    "    print(f\"Time to fit {k} kmeans clusters:{(time.time()-start_time)/60} minutes\")\n",
    "\n",
    "    #Get the labels for the cluster of each pose.\n",
    "    cluster_labels = clusters.labels_\n",
    "\n",
    "    #Compute the mean pose for each cluster.\n",
    "    #Initialize dictionary of empty lists with keys for each cluster.\n",
    "    pose_clusters = {}\n",
    "    number_of_clusters = k\n",
    "    for cluster_n in range(number_of_clusters):\n",
    "        pose_clusters[cluster_n] = []\n",
    "    #Fill dictionary with list of arrays in cluster.\n",
    "    for i in range(cluster_labels.shape[0]):\n",
    "        pose_clusters[cluster_labels[i]].append(dataset[i].tolist())\n",
    "\n",
    "    #Render a pose for each cluster and save the array.\n",
    "    pose_vocab = {}\n",
    "    for i in (range(number_of_clusters)):\n",
    "        cluster_mean = np.mean(np.array(pose_clusters[i]),axis=0)\n",
    "        pose_vocab[i] = cluster_mean.tolist()\n",
    "        pose = cluster_mean.reshape(14,2)\n",
    "        img = vis_single(pose, 'rendered/outfile_'+str(i)+'.jpg')\n",
    "\n",
    "    #Save vocab (means) to json dictionary.\n",
    "    with open(f'pose_vocab_{dataset_name}_{k}.json', 'w') as fp:\n",
    "        json.dump(pose_vocab, fp)\n",
    "    print(f'vocab poses saved to pose_vocab_{dataset_name}_{k}.json')\n",
    "    #Save clusters (all poses) to json dictionary\n",
    "    with open(f'pose_clusters_{dataset_name}_{k}.json', 'w') as fp:\n",
    "        json.dump(pose_clusters, fp)\n",
    "    print(f'all poses with labeled clusters saved to pose_clusters_{dataset_name}_{k}.json')\n",
    "\n",
    "    #Render pose vocabulary to video.\n",
    "    outfile_name = f'mean_poses_{dataset_name}_{k}'\n",
    "    fps = 2\n",
    "    images = glob.glob(\"rendered/*.jpg\")\n",
    "    make_video(images, outvid = f'{outfile_name}.avi',fps=fps)\n",
    "    print(f'video of pose vocab saved to {outfile_name}.avi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of raw data: (901500, 28)\n"
     ]
    }
   ],
   "source": [
    "#Import the D2M hiphop poses.\n",
    "d2m_hiphop = np.load('../vids_d2m_hiphop.npy')\n",
    "d2m_hiphop = d2m_hiphop.reshape(-1,28)\n",
    "print('shape of raw data:',d2m_hiphop.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of raw data: (6390600, 28)\n"
     ]
    }
   ],
   "source": [
    "#Import all D2M poses.\n",
    "vids_d2m_all = np.load('../vids_d2m_all.npy')\n",
    "vids_d2m_all = vids_d2m_all.reshape(-1,28)\n",
    "print('shape of raw data:',vids_d2m_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to fit 20 kmeans clusters:2.9488515575726826 minutes\n",
      "vocab poses saved to pose_vocab_[[288.34825397  44.82094737 288.34825397 ... 155.85936842 297.86583292\n",
      "  193.92968421]\n",
      " [288.07242941  47.26929825 288.07242941 ... 155.13519298 297.59000835\n",
      "  193.20550877]\n",
      " [286.79660485  46.54512281 286.79660485 ... 157.58354386 305.83176274\n",
      "  195.65385965]\n",
      " ...\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]\n",
      " [  0.           0.           0.         ...   0.           0.\n",
      "    0.        ]]_20.json\n",
      "all poses with labeled clusters saved to pose_clusters_d2m_hiphop_20.json\n",
      "video of pose vocab saved to mean_poses_d2m_hiphop_20.avi\n"
     ]
    }
   ],
   "source": [
    "get_mean_poses(d2m_hiphop, 'd2m_hiphop', k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 714. GiB for an array with shape (6390600, 15000) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-12385c2aeb0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_mean_poses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvids_d2m_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'd2m_all'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-c6ea8eff68a0>\u001b[0m in \u001b[0;36mget_mean_poses\u001b[0;34m(dataset, dataset_name, k)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mclusters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Time to fit {k} kmeans clusters:{(time.time()-start_time)/60} minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m                     \u001b[0mprecompute_distances\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute_distances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m                     x_squared_norms=x_squared_norms, random_state=seed)\n\u001b[0m\u001b[1;32m    938\u001b[0m                 \u001b[0;31m# determine if these results are the best so far\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbest_inertia\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minertia\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_inertia\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/cluster/_kmeans.py\u001b[0m in \u001b[0;36m_kmeans_single_elkan\u001b[0;34m(X, sample_weight, n_clusters, max_iter, init, verbose, x_squared_norms, random_state, tol, precompute_distances)\u001b[0m\n\u001b[1;32m    320\u001b[0m     centers, labels, n_iter = k_means_elkan(X, checked_sample_weight,\n\u001b[1;32m    321\u001b[0m                                             \u001b[0mn_clusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m                                             max_iter=max_iter, verbose=verbose)\n\u001b[0m\u001b[1;32m    323\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0minertia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcenters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32msklearn/cluster/_k_means_elkan.pyx\u001b[0m in \u001b[0;36msklearn.cluster._k_means_elkan.k_means_elkan\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 714. GiB for an array with shape (6390600, 15000) and data type float64"
     ]
    }
   ],
   "source": [
    "get_mean_poses(vids_d2m_all, 'd2m_all', k=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f36a22b0a532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'youtube'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'kmeans_model_{dataset_name}.sav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# save the model to disk\n",
    "dataset_name = 'youtube'\n",
    "filename = f'kmeans_model_{dataset_name}.sav'\n",
    "joblib.dump(clusters, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
