{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "\n",
    "from itertools import combinations_with_replacement as combo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dictionaries we need for training and bopbot pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to take the saved model and labels from the full train set of data run on 53k clusters and create our data for the GPT2 model. We need to translate keypoint arrays into fake words and back. This requires several steps. They will need to be done in batches; they crashed the kernel when trying to run immediately after the model finished training.\n",
    "\n",
    "- use the cluster labels for our train_all data to calculate the mean pose for each cluster.\n",
    "  - save a dictionary with cluster label as key and mean pose as value `label_to_meanPose_50k.json`\n",
    "- Assign a word to each mean pose, aka, each cluster label. (Some clusters will be empty and will not need an assigned word.)\n",
    "  - first step is to create a set of 5-letter words - get this from previous notebook\n",
    "  - save two dictionaries because we need to go both directions:\n",
    "    - use the words as keys and cluster labels as values: `word_to_label_50k.json`\n",
    "    - use the cluster labels as keys and words as values: `label_to_word_50k.json`\n",
    "- pipeline of data needed to train gpt model; here, we just need the entire train_all dataset as a text file with one word for each pose and the `[EOS]` token replacing the zeros; research the format used for the gpt2-tf2 version\n",
    "  - use the cluster labels (`train_all_52000_cluster_labels.npy`) as keys to get the text word\n",
    "  - save word to list\n",
    "  - replace zeros cluster with `[EOS]` \n",
    "  - save to text file: `train_all_50k.txt`\n",
    "- pipeline from user input array to output array returned to user\n",
    "  - keypoint array -> cluster label (done with predict using saved model)\n",
    "  - cluster label -> mean pose (used to output rendered video of vocab poses); these are saved in `label_to_mean_poses_50k.npz`\n",
    "  - cluster label -> text word (these words get run through model)\n",
    "  - text word -> cluster label -> mean pose (this is the output of the model translated to mean pose arrays ready for rendering; unless we create a third dict, this will require two dicts to perform)\n",
    "- randomly test a few videos\n",
    "  - search for zeros arrays; keep the index of these (save for future use); sequential indices in this list will be the beginning and ending index of a single video\n",
    "  - render the original and the vocab video (do this for a handful of random frames, too\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 929303)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Upload the label array.\n",
    "ballet_labels = np.load('ballet_8000_cluster_labels.npy')\n",
    "ballet_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929303, 14, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Upload the train_all array.\n",
    "train_ballet = np.load('../../data_preproc/train_ballet.npy')\n",
    "train_ballet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the text vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a bunch of 5-letter \"words\".\n",
    "b = combo('abcdefghijklmnopqrstuvwxyz', 5)\n",
    "c = list(b)\n",
    "vocab_5letter = []\n",
    "for i in range(len(c)):\n",
    "    vocab_5letter.append(''.join(c[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the real words from the encoder. (This is the list of words from the tf1 GPT2 model.)\n",
    "with open('../GPT2/gpt-2/models/124M/encoder.json', 'r') as f:\n",
    "    gpt_vocab = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the words used in our 52k vocab.\n",
    "with open('../../data_preproc/word_to_label_50k.json', 'r') as f:\n",
    "    word_to_label_50k = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list of \"words\" that aren't in the real words from the encoder and\n",
    "# that aren't used in our full 52k vocab.\n",
    "new_vocab_5letter = []\n",
    "for i in range(len(vocab_5letter)):\n",
    "    if vocab_5letter[i] not in gpt_vocab.keys():\n",
    "        if vocab_5letter[i] not in word_to_label_50k.keys():\n",
    "            new_vocab_5letter.append(vocab_5letter[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dgtvy',\n",
       " 'deegw',\n",
       " 'fjjnx',\n",
       " 'irttx',\n",
       " 'bhlmv',\n",
       " 'aegjr',\n",
       " 'eoprx',\n",
       " 'ccdez',\n",
       " 'lqtvv',\n",
       " 'mprtu']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shuffle the \"words\" just in case this might make a difference for the tokenizer.\n",
    "random.shuffle(new_vocab_5letter)\n",
    "new_vocab_5letter[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dictionaries with text words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7222"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ballet_labels[0][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929303, 28)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reshape to 2-dims\n",
    "train_ballet = train_ballet.reshape(-1,28)\n",
    "train_ballet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "562"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 0\n",
    "#Find the label for the zeros array.\n",
    "for i in range(train_ballet.shape[0]):\n",
    "    if np.all(train_ballet[i] == np.zeros(28)):\n",
    "        index = i\n",
    "        break\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Find the cluster label for zeros.\n",
    "ballet_labels[0][562]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7993"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a dictionary of fake 5-letter words with keys that are the cluster labels.\n",
    "label_to_word_ballet = {}\n",
    "count = 0\n",
    "for label in ballet_labels[0]:\n",
    "    if label not in label_to_word_ballet.keys():\n",
    "        if label == 4:\n",
    "            label_to_word_ballet[int(label)] = '[EOS]'\n",
    "        else:\n",
    "            label_to_word_ballet[int(label)] = new_vocab_5letter[count]\n",
    "            count += 1\n",
    "    \n",
    "len(label_to_word_ballet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7993"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create the inverse dictionary with words as keys and labels as values.\n",
    "word_to_label_ballet = {}\n",
    "for key in label_to_word_ballet:\n",
    "    new_key = label_to_word_ballet[key]\n",
    "    word_to_label_ballet[new_key] = int(key)\n",
    "len(word_to_label_ballet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coopq'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_word_ballet[2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_label_ballet['coopq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_label_ballet['[EOS]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the two dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('label_to_word_ballet.json','w') as f:\n",
    "    json.dump(label_to_word_ballet, f)\n",
    "with open('word_to_label_ballet.json','w') as moo:\n",
    "    json.dump(word_to_label_ballet, moo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the ballet8000 text file\n",
    "\n",
    "Based on the input used for this model, we want sentences on a single line with extra newlines at the end of paragraphs. I will use each video as a sentence and have each paragraph be just one sentence long.\n",
    "\n",
    "To run this, we need the `ballet_labels.npy` array and the `label_to_word_ballet` dictionary. [These are in memory, so don't need to reload.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35258430,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ##Upload the label array.\n",
    "# train_all_labels = np.load('train_all_52000_cluster_labels.npy').reshape(-1)\n",
    "# train_all_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51779"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Upload the label to word dictionary.\n",
    "# with open('label_to_word_50k.json', 'r') as f:\n",
    "#     label_to_word_50k = json.load(f)\n",
    "# len(label_to_word_50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929303, 1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ballet_labels = ballet_labels.reshape(-1,1)\n",
    "ballet_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2302"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ballet_labels[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of videos in text: 1396\n",
      "one video sentence: bikmm qstvv mnstw blmmt dlosw dlosw iijkn iijkn iijkn cccjt iijkn goruu goruu goruu goruu dmopv dmopv kknop kknop hnvvz bcctt bbdnv bbdnv bbjuz bbjuz bglwz bglwz eimsz eimsz myzzz myzzz myzzz myzzz evwwz evwwz evwwz evwwz evwwz evwwz evwwz evwwz evwwz hqqst hqqst fmpwy jnprx jnprx hlprw hlprw hlprw hikpv hikpv hikpv bbjoq bbjoq bbjoq bbjoq bghqs bghqs dlryz dlryz dlryz dlryz dlryz bbjoq hptty bbjoq bbjoq bbjoq bbjoq efglw ggtux ggtux ggtux ggtux bnsuw bnsuw bnsuw bnsuw bnsuw bnsuw bnsuw bnsuw djxxx djxxx djxxx djxxx ccnuz flntx flntx flntx flntx giktv giktv eeryz ciqtt ciqtt ciqtt ciqtt ciqtt dgmtx dgmtx dgmtx dgmtx dgmtx dgmtx dgmtx dgmtx morss ffgqr morss morss morss morss morss morss morss morss morss morss morss morss morss morss morss morss morss morss morss fmmmu fmmmu fmmmu fmmmu fmmmu fmmmu llpwy ilooq hhhsw cimuz cimuz achir asyyy aehir bgppt ghkww ghkww ghkww iijkn iijkn iijkn iijkn goruu goruu goruu dmopv dmopv kknop kknop eeoqz bcctt bcctt bbdnv bbjuz bbjuz bglwz eimsz eimsz myzzz iijkn goruu goruu goruu bbdnv bbdnv bbjuz bglwz myzzz hqqst evwwz evwwz evwwz evwwz evwwz evwwz evwwz evwwz evwwz hqqst fmpwy fmpwy jnprx hlprw hlprw hlprw bbjoq bbjoq bbjoq bbjoq bghqs bghqs dlryz dlryz dlryz dlryz dlryz bbjoq bbjoq bbjoq bbjoq bbjoq bbjoq efglw bnsuw bnsuw bnsuw bnsuw bnsuw bnsuw bnsuw bggmq djxxx djxxx djxxx djxxx ccnuz flntx flntx flntx giktv giktv myzzz hqqst evwwz evwwz evwwz evwwz hqqst hqqst mmmmq jnprx jnprx hlprw hlprw hikpv hikpv hikpv bbjoq bbjoq bbjoq bbjoq bghqs bghqs dlryz dlryz dlryz bbjoq bbjoq bbjoq efglw ggtux ggtux ggtux bnsuw bnsuw bnsuw bnsuw bnsuw bnsuw bggmq djxxx djxxx djxxx djxxx giktv giktv giktv ciqtt ciqtt ciqtt ciqtt ciqtt dgmtx dgmtx dgmtx dgmtx dgmtx dgmtx dgmtx dgmtx dgmtx morss morss morss morss [EOS] \n"
     ]
    }
   ],
   "source": [
    "#We want to scroll through the label array and add \n",
    "# the word for each label to new_sentence until we reach \"[EOS]\" (label 21).\n",
    "#Then we append the sentence to the all_text list.\n",
    "#Final data will be a list of strings to save to txt file with each new string\n",
    "#starting on a new line.\n",
    "\n",
    "all_text = []\n",
    "new_sentence = ''\n",
    "for i in range(ballet_labels.shape[0]):\n",
    "    new_sentence += label_to_word_ballet[ballet_labels[i][0]]+' '\n",
    "    if ballet_labels[i][0] == 4:\n",
    "        all_text.append(new_sentence)\n",
    "        new_sentence = ''\n",
    "print('number of videos in text:',len(all_text))\n",
    "print('one video sentence:',all_text[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'train_ballet_8k.txt'\n",
    "with open(filename, \"w\") as whatevs:\n",
    "    for item in all_text:\n",
    "        whatevs.write('%s\\n' % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a sample of text from current GPT2 input:\n",
    "\n",
    "    Governor Cuomo was angling for more federal funds way back in July of 2015:\n",
    "\n",
    "    Israel News reports:\n",
    "\n",
    "    New rail tunnels under the Hudson River are needed to reduce delays, but the expensive project won’t work without a greater financial commitment from the federal government, Gov. Andrew Cuomo said Wednesday.\n",
    "\n",
    "    The Democrat said Washington’s proposal to cover $3 billion of the estimated $14 billion tunnel project isn’t enough. He and New Jersey Gov. Chris Christie are expected to meet soon with federal officials to discuss funding for the stalled plan, which comes after a series of delays that underscored the age and condition of the area’s transportation infrastructure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
